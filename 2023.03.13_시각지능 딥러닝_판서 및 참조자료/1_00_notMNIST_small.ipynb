{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"B_iAjwPm_mYg"},"source":["# Letter recognition (small size)\n","\n","> Indeed, I once even proposed that the toughest challenge facing AI workers is to answer the question: “What are the letters ‘A’ and ‘I’? - [Douglas R. Hofstadter](https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html) (1995)\n","\n","\n","## notMNIST\n","\n","\n","Data source: [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) (you need to download `notMNIST_small.mat` file):\n","\n","![](http://yaroslavvb.com/upload/notMNIST/nmn.png)\n","\n","> some publicly available fonts and extracted glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J taken from different fonts.\n","\n","> Approaching 0.5% error rate on notMNIST_small would be very impressive. If you run your algorithm on this dataset, please let me know your results.\n","\n","\n","## So, why not MNIST?\n","\n","Many introductions to image classification with deep learning start with MNIST, a standard dataset of handwritten digits. This is unfortunate. Not only does it not produce a “Wow!” effect or show where deep learning shines, but it also can be solved with shallow machine learning techniques. In this case, plain k-Nearest Neighbors produces more than 97% accuracy (or even 99.5% with some data preprocessing!). Moreover, MNIST is not a typical image dataset – and mastering it is unlikely to teach you transferable skills that would be useful for other classification problems\n","\n","> Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad ideas may work on MNIST and no[t] transfer to real [computer vision]. - [François Chollet’s tweet](https://twitter.com/fchollet/status/852594987527045120)"]},{"cell_type":"code","metadata":{"id":"jcAAphar__K6"},"source":["!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bJ8z5dnANsf"},"source":["import matplotlib.pyplot as plt\n","from scipy import io\n","import numpy as np\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rsvxhuP0_mYt"},"source":["## Data Loading"]},{"cell_type":"code","metadata":{"id":"wb9UaA_P_mYu"},"source":["data = io.loadmat(\"notMNIST_small.mat\")\n","\n","# transform data\n","X = data['images']\n","y = data['labels']\n","resolution = 28\n","classes = 10\n","\n","X = np.transpose(X, (2, 0, 1))\n","\n","y = y.astype('int32')\n","X = X.astype('float32') / 255.\n","\n","# shape: (sample, x, y, channel)\n","X = X.reshape((-1, resolution, resolution, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHYFpZ0L_mYw"},"source":["# looking at data; some fonts are strange\n","i = np.random.randint(0, 18724)\n","\n","plt.imshow(X[i,:,:,0])\n","plt.title(\"ABCDEFGHIJ\"[y[i]]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeeDSEJv_mYz"},"source":["# random letters\n","rows = 6\n","fig, axs = plt.subplots(rows, classes, figsize=(classes, rows))\n","\n","for letter_id in range(10):\n","    letters = X[y == letter_id]\n","    for i in range(rows):\n","        ax = axs[i, letter_id]\n","        ax.imshow(letters[np.random.randint(len(letters)),:,:,0],\n","                  cmap='Greys', interpolation='none')\n","        ax.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xmR99PK_mY2"},"source":["# splitting data into training and test sets\n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzL95VmTBHxx"},"source":["x_train.shape, y_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras"],"metadata":{"id":"_KFuIE0Rlfla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"NA8B32uEljRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_n = len(np.unique(y_train))"],"metadata":{"id":"6avp8BSIlmGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train = to_categorical(y_train, class_n)\n","y_test = to_categorical(y_test, class_n)"],"metadata":{"id":"jZzH0IrVlw9T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train.shape, y_train.shape"],"metadata":{"id":"Zm8F8_dgl2sY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnZHnb1uBJWj"},"source":["# Keras를 이용한 모델링 훈련!\n","\n","1. Flatten layer 활용\n","2. activation이 주어진 Dense layer뒤에 BatchNormalization둬볼 것\n","3. Dropout을 0.2 정도로 활용해볼 것\n","4. Early stopping도 활용해볼 것"]},{"cell_type":"code","source":["## Sequential API\n","# 1번. 세션 클리어 : 기존의 모델 구조가 메모리에 남아있다면 지워줘\n","keras.backend.clear_session()\n","\n","# 2. 모델 발판 선언 : 레이어 블록을 차곡차곡 쌓을!\n","model = keras.models.Sequential()\n","\n","# 3. 레이어 조립 : .add( )\n","model.add( keras.layers.Input(shape=(28,28,1)) )\n","model.add( keras.layers.Flatten() )\n","\n","model.add( keras.layers.Dense(512, activation='relu') )\n","model.add( keras.layers.Dense(512, activation='relu') )\n","model.add( keras.layers.BatchNormalization() )\n","model.add( keras.layers.Dropout(0.2) )\n","\n","model.add( keras.layers.Dense(256, activation='relu') )\n","model.add( keras.layers.Dense(256, activation='relu') )\n","model.add( keras.layers.BatchNormalization() )\n","model.add( keras.layers.Dropout(0.2) )\n","\n","model.add( keras.layers.Dense(10, activation='softmax') )\n","\n","# 4. 컴파일\n","model.compile(loss=keras.losses.categorical_crossentropy, metrics=['accuracy'],\n","              optimizer='adam')\n","\n","# 요약\n","model.summary()"],"metadata":{"id":"wF_-Qy9x6qva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Functional API\n","# 1번. 세션 클리어\n","keras.backend.clear_session()\n","\n","# 2번. 레이어 사슬처럼 엮기\n","il = keras.layers.Input(shape=(28,28,1))\n","fl = keras.layers.Flatten()(il)\n","\n","hl = keras.layers.Dense(512, activation='relu')(fl)\n","hl = keras.layers.Dense(512, activation='relu')(hl)\n","bl = keras.layers.BatchNormalization()(hl)\n","dl = keras.layers.Dropout(0.2)(bl)\n","\n","hl = keras.layers.Dense(256, activation='relu')(dl)\n","hl = keras.layers.Dense(256, activation='relu')(hl)\n","bl = keras.layers.BatchNormalization()(hl)\n","dl = keras.layers.Dropout(0.2)(bl)\n","\n","ol = keras.layers.Dense(10, activation='softmax')(dl)\n","\n","# 3번. 모델의 시작/끝 지정\n","model = keras.models.Model(il, ol)\n","\n","# 4번. 컴파일\n","model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n","              optimizer='adam')\n","\n","# 요약\n","model.summary()"],"metadata":{"id":"kxiB4Xhwt8j7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Early Stopping\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"8KixauQSqi1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = EarlyStopping(monitor='val_loss',         ## 관측 대상 : 얼리스토핑을 무엇을 보고 적용할래\n","                   min_delta=0,                ## 임계값\n","                   patience=5,                 ## 관측 대상의 성능이 개선되지 않았을 때, 몇 번 더 지켜볼래\n","                   verbose=1,\n","                   restore_best_weights=True)  ## 학습하고 나서 최적의 가중치를 가진 epoch의 가중치를 적용!"],"metadata":{"id":"DSbDNYcYs2sI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(x_train,y_train,\n","          validation_split=0.2,\n","          epochs=10000,verbose=1,\n","          callbacks=[es])"],"metadata":{"id":"tIJnilc1tR7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"828saEvytcZt"},"execution_count":null,"outputs":[]}]}